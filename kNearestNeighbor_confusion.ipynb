{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce1f7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from processing_functions import ppt_id, create_epochs, freq_ind, create_numeric_labels, relative_band_power\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c31269",
   "metadata": {},
   "source": [
    "The following piece of code creates numerical labels for the target variable: 0 for Alzheimer's, 1 for Frontotemporal dementia, and 2 for healthy group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51b45a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_diagnostics = pd.read_csv('data/ds004504/participants.tsv',sep='\\t')\n",
    "target_labels = ppt_diagnostics['Group'].apply(create_numeric_labels).values\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09f381",
   "metadata": {},
   "source": [
    "The general pipeline is to import the data for the patient, create epochs of 4 s each with 50 percent overlap, then compute the average power spectrum for each of the five bands for all 19 channels, so we end up with $19*5 = 95$ features for each epoch. We then train a three nearest neighbor classifier on the patients. We choose leave one patient out cross validation, so we will train on all but one patients and test the accuracy of the method on this last patient. We record the accuracy of every run and then in the end, calculate the mean and the standard deviation of the procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6769bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_length = 2000\n",
    "overlap_ratio = 0.5\n",
    "freq_bands = np.array([0.5,4.0,8.0,13.0,25.0,45.0])\n",
    "sample_freq = 500 #hertz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca64b5",
   "metadata": {},
   "source": [
    "The code below goes through all the participants and creates a list of features and target variables, which we will then use for training and validating our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89f6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "targets = []\n",
    "for i in range(len(target_labels)):\n",
    "    ppt = i + 1\n",
    "    raw_data = mne.io.read_raw_eeglab('data/ds004504/derivatives/' + ppt_id(ppt)\n",
    "                                  + '/eeg/' + ppt_id(ppt) + '_task-eyesclosed_eeg.set', preload = True)\n",
    "    export = raw_data.to_data_frame()\n",
    "    ppt_array = export.iloc[:,range(1,len(export.columns))].values\n",
    "    del raw_data\n",
    "    del export\n",
    "    ppt_epochs = create_epochs(ppt_array,epoch_length,overlap_ratio)\n",
    "    freqs, ppt_psd  = welch(ppt_epochs,fs=sample_freq, axis=1)\n",
    "    ppt_rbp = relative_band_power(ppt_psd,freqs,freq_bands)\n",
    "    ppt_rbp_reshaped = ppt_rbp.reshape((ppt_rbp.shape[0], -1))\n",
    "    features += [ppt_rbp_reshaped]\n",
    "    targets += [[target_labels[i]]*ppt_rbp.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b55e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 88\n",
      "(383, 95) 383\n"
     ]
    }
   ],
   "source": [
    "# Just checking that the features and targets have the same shape. \n",
    "print(len(features), len(targets))\n",
    "print(features[10].shape, len(targets[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21ca0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.concatenate(features[:1] + features[2:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f7733",
   "metadata": {},
   "source": [
    "We start by just training a two class classifier for Alzheimer's vs Healthy. It is easy to see that the first 65 patients correspond to those two classes so we will first use those two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b05b3eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_valid_acc = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1s = []\n",
    "for i in range(20):\n",
    "    # The following two arrays will store the target and features for the train set. We will append to it\n",
    "    # as we go through various subjects. \n",
    "    train_X = np.concatenate(features[:i] + features[i+1:25] + features[25:36+i] + features[36 + i + 1:65])\n",
    "    train_y = np.concatenate(targets[:i] + targets[i+1:25] + targets[25:36+i] + targets[36 + i + 1:65])\n",
    "    \n",
    "    \n",
    "    # The following two arrays will store target and features for the test set. \n",
    "    test_X = np.concatenate((features[i],features[36+i]))\n",
    "    test_y = np.concatenate((targets[i],targets[36+i]))\n",
    "    \n",
    "    # scaling the features to make sure they have same mean and standard deviation\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    \n",
    "    # We now train the classifier on the test data\n",
    "    ThreeNN = KNeighborsClassifier(n_neighbors=3)\n",
    "    ThreeNN.fit(train_X, train_y)\n",
    "    \n",
    "    \n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    # This stores the accuracy on the test data\n",
    "    cross_valid_acc += [ThreeNN.score(test_X, test_y)]\n",
    "    sensitivities += [recall_score(test_y, ThreeNN.predict(test_X), labels = [0,2], pos_label = 0)]\n",
    "    specificities += [recall_score(test_y, ThreeNN.predict(test_X), labels = [0,2],pos_label = 2)]\n",
    "    f1s += [f1_score(test_y, ThreeNN.predict(test_X),labels = [0,2],pos_label=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f9fb282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6308724832214765,\n",
       " 0.47848101265822784,\n",
       " 0.5,\n",
       " 0.9147727272727273,\n",
       " 0.5411471321695761,\n",
       " 0.19365079365079366,\n",
       " 0.8272251308900523,\n",
       " 0.9012658227848102,\n",
       " 0.3836065573770492,\n",
       " 0.14084507042253522]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_valid_acc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "389738c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6717040599759079, 0.1397259950247155)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_valid_acc), np.std(cross_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "127c878c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6409631722951434, 0.22627624017984624)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sensitivities), np.std(sensitivities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ffda7c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6959669606166726, 0.18708500497032324)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(specificities), np.std(specificities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8540e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6335009471229563, 0.18467423997422325)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(f1s), np.std(f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851b83c",
   "metadata": {},
   "source": [
    "Now we train a three class classifier on the same data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af2fdd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_valid_acc_3class = []\n",
    "for i in range(len(target_labels)):\n",
    "    \n",
    "    # The following two arrays will store the target and features for the train set. We will append to it\n",
    "    # as we go through various subjects. \n",
    "    train_X = np.concatenate(features[:i] + features[i+1:])\n",
    "    train_y = np.concatenate(targets[:i] + targets[i+1:])\n",
    "    \n",
    "    \n",
    "    # The following two arrays will store target and features for the test set. \n",
    "    test_X = features[i]\n",
    "    test_y = targets[i]\n",
    "    \n",
    "    # scaling the features to make sure they have same mean and standard deviation\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    \n",
    "    # We now train the classifier on the test data\n",
    "    ThreeNN = KNeighborsClassifier(n_neighbors=3)\n",
    "    ThreeNN.fit(train_X, train_y)\n",
    "    \n",
    "    \n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    # This stores the accuracy on the test data\n",
    "    cross_valid_acc_3class += [ThreeNN.score(test_X, test_y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b699a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5637583892617449,\n",
       " 0.35443037974683544,\n",
       " 0.4342105263157895,\n",
       " 0.4914772727272727,\n",
       " 0.513715710723192,\n",
       " 0.17777777777777778,\n",
       " 0.599476439790576,\n",
       " 0.8025316455696202,\n",
       " 0.3770491803278688,\n",
       " 0.1674491392801252]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_valid_acc_3class[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3840e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.45720000208854844, 0.25162364663428294)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_valid_acc_3class), np.std(cross_valid_acc_3class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d1e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
